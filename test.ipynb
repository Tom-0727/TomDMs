{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=1, out_channels=1, Transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_size = (1, 1, 572, 572)\n",
    "test_tensor = torch.randn(tensor_size)\n",
    "opt = model(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "\n",
    "model = VanillaVAE(\n",
    "    in_channels=3,\n",
    "    latent_dim=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Batch_size x Channels x Height x Width\n",
    "tensor_size = (3, 3, 28, 28)\n",
    "tensor = torch.randn(tensor_size)\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = model.encode(tensor)\n",
    "print(mu.size())\n",
    "print(log_var.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.encode(tensor)\n",
    "print(result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.flatten(result, start_dim=1)\n",
    "print(test.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import make_s_curve\n",
    "\n",
    "s_curve, _ = make_s_curve(n_samples=10**4, noise=0.1)\n",
    "s_curve = s_curve[:, [0, 2]]/10.0  # shape of (10000, 2)\n",
    "\n",
    "data = s_curve.T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*data, color='black')\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "dataset = torch.Tensor(s_curve).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.randn((10000, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_steps = 100 # could estimate by beta, mu and sigma\n",
    "\n",
    "betas = torch.linspace(-6, 6, num_steps)  # evenly spaced between -6 and 6\n",
    "betas = torch.sigmoid(betas) * (0.5e-2 - 1e-5) + 1e-5  # 1e-5 to 0.5e-2  where sigmoid opt 0 - 1\n",
    "\n",
    "alphas = 1 - betas  \n",
    "alphas_prod = torch.cumprod(alphas, dim=0)  # cumulative product\n",
    "#alphas_prod_prev = torch.cat([torch.tensor([1.0]), alphas_prod[:-1]], 0)  # concatenate 1.0 at the beginning\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)  \n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "\n",
    "assert alphas.shape==alphas_prod.shape==alphas_bar_sqrt.shape==one_minus_alphas_bar_sqrt.shape\n",
    "print(betas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2, 2, 2, 2])\n",
    "cp_x = torch.cumprod(x, dim=0)\n",
    "print(cp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x_t in any t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given x_0, t; Get any x_t using reparameterization trick\n",
    "def q_x(x_0, t):\n",
    "    noise = torch.randn_like(x_0)  # get from N(0,1)\n",
    "    alpha_bar_t = alphas_bar_sqrt[t]  # get the aplha value at time t (sigma)\n",
    "    alpha_1_m_t = one_minus_alphas_bar_sqrt[t]  \n",
    "    return (alpha_bar_t * x_0 + alpha_1_m_t * noise)  # mu + sigma * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the noising opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shows = 20\n",
    "fig, axs = plt.subplots(2, 10, figsize=(28, 3))\n",
    "plt.rc('text', color='blue')\n",
    "\n",
    "for i in range(num_shows):\n",
    "    j = i // 10\n",
    "    k = i % 10\n",
    "\n",
    "    q_i = q_x(dataset, torch.tensor([i*num_steps//num_shows]))  # Add i steps noise\n",
    "    \n",
    "    axs[j,k].scatter(q_i[:,0], q_i[:,1], color='black')\n",
    "    axs[j,k].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPDiffuser(nn.Module):\n",
    "    def __init__(self, n_steps, num_units=128):\n",
    "        super(MLPDiffuser, self).__init__()\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(2, num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units, num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units, num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units, 2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.step_embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(n_steps, num_units),\n",
    "                nn.Embedding(n_steps, num_units),\n",
    "                nn.Embedding(n_steps, num_units),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_0, t):\n",
    "        x = x_0\n",
    "        for i, layer in enumerate(self.step_embeddings):\n",
    "            t_embeddings = layer(t)\n",
    "            x = self.linears[2*i](x)\n",
    "            x += t_embeddings\n",
    "            x = self.linears[2*i+1](x)\n",
    "        \n",
    "        x = self.linears[-1](x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model, x_0, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, n_steps):\n",
    "    batch_size = x_0.shape[0]\n",
    "\n",
    "    t = torch.randint(0, n_steps, size=(batch_size//2,))\n",
    "    t = torch.cat([t, n_steps - t - 1], 0)\n",
    "    t = t.unsqueeze(-1)\n",
    "\n",
    "    a = alphas_bar_sqrt[t]\n",
    "\n",
    "    am1 = one_minus_alphas_bar_sqrt[t]\n",
    "\n",
    "    e = torch.randn_like(x_0)\n",
    "\n",
    "    x = x_0 * a + am1 * e\n",
    "\n",
    "    opt = model(x, t.squeeze(-1))\n",
    "\n",
    "    return (e-opt).square().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample(model, x, t, betas, one_minus_alphas_bar_sqrt):\n",
    "    t = torch.tensor([t])\n",
    "    \n",
    "    coeff = betas[t] / one_minus_alphas_bar_sqrt[t]\n",
    "\n",
    "    eps_theta = model(x, t)\n",
    "\n",
    "    mean = (1/(1-betas[t]).sqrt()) * (x - coeff * eps_theta)\n",
    "\n",
    "    z = torch.randn_like(x)\n",
    "    sigma_ = betas[t].sqrt()\n",
    "\n",
    "    sample = mean + sigma_ * z\n",
    "\n",
    "    return (sample)\n",
    "\n",
    "def p_sample_loop(model, shape, num_steps, betas, one_minus_alphas_bar_sqrt):\n",
    "    cur_x = torch.randn(shape)\n",
    "\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(num_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt)\n",
    "\n",
    "        x_seq.append(cur_x)\n",
    "    \n",
    "    return x_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 128\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "num_epoch = 4000\n",
    "\n",
    "model = MLPDiffuser(num_steps)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    for idx, batch_x in enumerate(dataloader):\n",
    "        loss = diffusion_loss_fn(model, batch_x, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, num_steps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    if (t%100 == 0):\n",
    "        print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = p_sample_loop(model, dataset.shape, num_steps, betas, one_minus_alphas_bar_sqrt)\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(28, 3))\n",
    "for i in range(1, 11):\n",
    "    cur_x = x_seq[i * 10].detach()\n",
    "    axs[i-1].scatter(cur_x[:,0], cur_x[:,1], color='black')\n",
    "    axs[i-1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './MLPDiffuser.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpm import *\n",
    "\n",
    "mlp_trainer = MLPDiffuserTrainer(\n",
    "    num_steps = 100,\n",
    "    num_epoch = 4000,\n",
    "    gpu = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_trainer.train()\n",
    "mlp_trainer.save(\n",
    "    path='./MLPDiffuser.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_trainer.load(\n",
    "    path = './MLPDiffuser.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_trainer.demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomdms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
